"""
Code generator: Plan -> Python code (template-first approach)
"""
import os
import json
from typing import Dict, Any, List, Tuple
from jinja2 import Template
from backend.services.codegen.prompt_templates_v2 import CODE_GENERATION_TEMPLATE, CODE_EXPLANATION_TEMPLATE
from backend.utils.logging import setup_logger

logger = setup_logger(__name__)


class CodeGenerator:
    """Generate Python analysis code from plan"""
    
    def __init__(self, openai_api_key: str = None, openai_base_url: str = None, llm_model: str = "gpt-4"):
        """
        Initialize code generator
        
        Args:
            openai_api_key: OpenAI API key
            openai_base_url: OpenAI API base URL
            llm_model: LLM model name
        """
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self.openai_base_url = openai_base_url or os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.llm_model = llm_model
    
    def generate_template_code(self, plan: Dict[str, Any], question: str, columns: List[str] = None) -> str:
        """
        Generate code using templates (faster, more reliable)
        
        Args:
            plan: Executable plan
            question: Original question
            
        Returns:
            Python code string
        """
        logger.info("Generating code from template")
        
        file_name = plan['file_name']
        sheet_name = plan['sheet_name']
        
        # Start with imports and data loading
        code_lines = [
            "import pandas as pd",
            "import numpy as np",
            "import plotly.graph_objects as go",
            "import plotly.express as px",
            "import json",
            "import re",
            "from collections import Counter",
            "",
            "# è¾…åŠ©å‡½æ•°",
            "def normalize_numeric(series):",
            "    if series.dtype in [np.float64, np.int64]:",
            "        return series",
            "    s = series.astype(str)",
            "    s = s.str.replace(',', '').str.replace('ï¼Œ', '').str.replace(' ', '')",
            "    s = s.str.replace('Â¥', '').str.replace('$', '').str.replace('å…ƒ', '')",
            "    s = s.str.replace('%', '').str.replace('ï¼…', '')",
            "    return pd.to_numeric(s, errors='coerce')",
            "",
            "def extract_zh_keywords(text_series, topk=20):",
            "    all_text = ' '.join(text_series.dropna().astype(str))",
            "    words = re.findall(r'[\\u4e00-\\u9fff]+', all_text)",
            "    words = [w for w in words if len(w) >= 2]",
            "    counter = Counter(words)",
            "    return counter.most_common(topk)",
            "",
            f"# åŠ è½½æ•°æ®ï¼š{file_name} - {sheet_name}",
            "# æ³¨æ„ï¼šå®é™…æ‰§è¡Œæ—¶ï¼Œdfä¼šç”±æ‰§è¡Œå™¨ä¼ å…¥",
            "# df = pd.read_excel(...)",
            "",
        ]
        
        # Apply filters
        if plan.get('filters'):
            code_lines.append("# åº”ç”¨è¿‡æ»¤æ¡ä»¶")
            for f in plan['filters']:
                col = f['col']
                op = f['op']
                val = f['value']
                
                if op == '>=':
                    code_lines.append(f"df = df[df['{col}'] >= '{val}']")
                elif op == '<=':
                    code_lines.append(f"df = df[df['{col}'] <= '{val}']")
                elif op == '==':
                    code_lines.append(f"df = df[df['{col}'] == '{val}']")
                elif op == 'in':
                    code_lines.append(f"df = df[df['{col}'].isin({val})]")
            code_lines.append("")
        
        # Group-by and aggregation
        if plan.get('groupby') and plan.get('agg'):
            code_lines.append("# åˆ†ç»„èšåˆ")
            groupby_cols = plan['groupby']
            agg_dict = {}
            for agg_spec in plan['agg']:
                col = agg_spec['col']
                op = agg_spec['op']
                agg_dict[col] = op
            
            code_lines.append(f"result = df.groupby({groupby_cols}).agg({agg_dict}).reset_index()")
            code_lines.append("")
        
            # Trend analysis
        elif plan.get('trend'):
            trend = plan['trend']
            date_col = trend['date_col']
            freq = trend['freq']
            value_cols = trend.get('value_cols', [])
            
            code_lines.append("# è¶‹åŠ¿åˆ†æ")
            code_lines.append(f"df['{date_col}'] = pd.to_datetime(df['{date_col}'], errors='coerce')")
            code_lines.append(f"df = df.dropna(subset=['{date_col}'])")
            
            if freq == 'M':
                code_lines.append(f"df['period'] = df['{date_col}'].dt.strftime('%Y-%m')")
            elif freq == 'Y':
                code_lines.append(f"df['period'] = df['{date_col}'].dt.year.astype(str)")
            elif freq == 'D':
                code_lines.append(f"df['period'] = df['{date_col}'].dt.strftime('%Y-%m-%d')")
            else:
                code_lines.append(f"df['period'] = df['{date_col}'].astype(str)")
            
            if plan.get('groupby'):
                groupby_cols = ['period'] + plan['groupby']
            else:
                groupby_cols = ['period']
            
            # Try to find a value column if not specified
            if not value_cols:
                # Look for sales/revenue/amount columns
                for col in columns:
                    if 'é”€å”®é¢' in col or 'æ€»é”€å”®é¢' in col:
                        value_cols = [col]
                        break
                    elif 'é”€é‡' in col or 'é”€å”®æ•°é‡' in col:
                        value_cols = [col]
                        break
            
            if value_cols:
                agg_dict = {col: 'sum' for col in value_cols}
                code_lines.append(f"result = df.groupby({groupby_cols}).agg({agg_dict}).reset_index()")
            else:
                # Fallback if no value columns found
                code_lines.append(f"result = df.groupby({groupby_cols}).size().reset_index(name='count')")
            
            code_lines.append("")
        
        # Text analysis
        elif plan.get('text_ops'):
            text_ops = plan['text_ops']
            text_col = text_ops['text_col']
            topk = text_ops.get('topk', 20)
            
            code_lines.append("# æ–‡æœ¬å…³é”®è¯æå–")
            code_lines.append(f"keywords = extract_zh_keywords(df['{text_col}'], topk={topk})")
            code_lines.append("result = pd.DataFrame(keywords, columns=['å…³é”®è¯', 'é¢‘æ¬¡'])")
            code_lines.append("")
        
        # Simple aggregation without group-by
        elif plan.get('agg') and not plan.get('groupby'):
            code_lines.append("# èšåˆç»Ÿè®¡")
            for agg_spec in plan['agg']:
                col = agg_spec['col']
                op = agg_spec['op']
                code_lines.append(f"result_{op} = df['{col}'].{op}()")
                code_lines.append(f"print(f'{col} {op}: {{result_{op}}}')")
            code_lines.append("result = df")  # Keep original data
            code_lines.append("")
        
        else:
            code_lines.append("result = df")
            code_lines.append("")
        
        # Sort
        if plan.get('sort'):
            code_lines.append("# æ’åº")
            for sort_spec in plan['sort']:
                col = sort_spec['col']
                order = sort_spec['order']
                ascending = (order == 'asc')
                code_lines.append(f"result = result.sort_values('{col}', ascending={ascending})")
            code_lines.append("")
        
        # Limit
        if plan.get('limit'):
            code_lines.append(f"# å–å‰{plan['limit']}æ¡")
            code_lines.append(f"result = result.head({plan['limit']})")
            code_lines.append("")
        
        # Visualization
        viz_type = plan.get('viz', 'table')
        if viz_type == 'line' and plan.get('trend'):
            code_lines.append("# ç”ŸæˆæŠ˜çº¿å›¾")
            code_lines.append("fig = go.Figure()")
            
            if plan.get('groupby') and len(plan['groupby']) > 0:
                group_col = plan['groupby'][0]
                value_cols = plan['trend'].get('value_cols', [])
                if value_cols:
                    value_col = value_cols[0]
                    code_lines.append(f"for group_name in result['{group_col}'].unique():")
                    code_lines.append(f"    group_data = result[result['{group_col}'] == group_name]")
                    code_lines.append(f"    fig.add_trace(go.Scatter(")
                    code_lines.append(f"        x=group_data['period'],")
                    code_lines.append(f"        y=group_data['{value_col}'],")
                    code_lines.append(f"        mode='lines+markers',")
                    code_lines.append(f"        name=str(group_name)")
                    code_lines.append(f"    ))")
            else:
                value_cols = plan['trend'].get('value_cols', [])
                if value_cols:
                    value_col = value_cols[0]
                    code_lines.append(f"fig.add_trace(go.Scatter(")
                    code_lines.append(f"    x=result['period'],")
                    code_lines.append(f"    y=result['{value_col}'],")
                    code_lines.append(f"    mode='lines+markers'")
                    code_lines.append(f"))")
            
            code_lines.append("fig.update_layout(title='è¶‹åŠ¿åˆ†æ', xaxis_title='æ—¶é—´', yaxis_title='æ•°å€¼')")
            code_lines.append("print('PLOTLY_JSON:', fig.to_json())")
            code_lines.append("")
        
        elif viz_type == 'bar':
            code_lines.append("# ç”ŸæˆæŸ±çŠ¶å›¾")
            if plan.get('groupby') and plan.get('agg'):
                x_col = plan['groupby'][0]
                y_col = plan['agg'][0]['col']
                code_lines.append(f"fig = px.bar(result, x='{x_col}', y='{y_col}', title='æŸ±çŠ¶å›¾')")
                code_lines.append("print('PLOTLY_JSON:', fig.to_json())")
                code_lines.append("")
        
        # Print result table
        code_lines.append("# è¾“å‡ºç»“æœ")
        code_lines.append("print('\\n=== åˆ†æç»“æœ ===')")
        code_lines.append("print(result.head(20).to_string(index=False))")
        code_lines.append("print(f'\\næ€»è¡Œæ•°: {len(result)}')")
        
        code = '\n'.join(code_lines)
        return code
    
    def generate_with_llm(self, plan: Dict[str, Any], question: str,
                         columns: List[str], types: Dict[str, str],
                         data_sample: str = "", row_count: int = 0,
                         excel_path: str = "",
                         other_sheets: List[Dict] = None,
                         original_file: str = "") -> Tuple[str, str]:
        """
        Generate code using LLM (V2 - Simplified)
        
        Args:
            plan: Executable plan
            question: Original question
            columns: Available columns
            types: Column types
            data_sample: Sample data rows (head + tail)
            row_count: Total row count
            excel_path: Path to clean Excel file
            other_sheets: Other sheets from same file
            original_file: Original file name
            
        Returns:
            Tuple of (code, prompt)
        """
        logger.info("Generating code with LLM (V2 - Simplified)")
        
        # Handle defaults
        if other_sheets is None:
            other_sheets = []
        
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=self.openai_api_key, base_url=self.openai_base_url)
            
            # Format column info (name + dtype)
            column_info_lines = []
            for col in columns:
                dtype = types.get(col, 'object')
                column_info_lines.append(f"  {col}: {dtype}")
            column_info = '\n'.join(column_info_lines)
            
            # Split data sample into head and tail
            if "ã€å‰5è¡Œæ•°æ®ã€‘" in data_sample:
                parts = data_sample.split("ã€å‰5è¡Œæ•°æ®ã€‘")[1].split("ã€å5è¡Œæ•°æ®ã€‘")
                head_sample = parts[0].strip() if len(parts) > 0 else data_sample
                tail_sample = parts[1].strip() if len(parts) > 1 else ""
            else:
                head_sample = data_sample
                tail_sample = ""
            
            # Build other sheets info
            other_sheets_info = ""
            if other_sheets and len(other_sheets) > 0:
                other_sheets_info = f"\nã€åŒä¸€æ–‡ä»¶çš„å…¶ä»–å·¥ä½œè¡¨ã€‘ï¼ˆåŸå§‹æ–‡ä»¶ï¼š{original_file}ï¼Œå…±{len(other_sheets)+1}ä¸ªå·¥ä½œè¡¨ï¼‰\n\n"
                other_sheets_info += "å¦‚æœåˆ†æéœ€è¦ï¼Œå¯ä»¥åŠ è½½å¹¶åˆå¹¶è¿™äº›å·¥ä½œè¡¨çš„æ•°æ®ï¼š\n\n"
                
                for i, sheet_info in enumerate(other_sheets, 1):
                    sheet_name_other = sheet_info['sheet_name']
                    sheet_path = sheet_info['excel_path']
                    sheet_shape = sheet_info['shape']
                    sheet_cols = sheet_info['columns']
                    sheet_head = sheet_info.get('head_sample', '')[:300]  # Limit length
                    
                    other_sheets_info += f"å·¥ä½œè¡¨ {i}: {sheet_name_other}\n"
                    other_sheets_info += f"  - è·¯å¾„: {sheet_path}\n"
                    other_sheets_info += f"  - å¤§å°: {sheet_shape[0]}è¡Œ Ã— {sheet_shape[1]}åˆ—\n"
                    other_sheets_info += f"  - åˆ—å: {', '.join(sheet_cols[:10])}\n"
                    if sheet_head:
                        other_sheets_info += f"  - æ•°æ®é¢„è§ˆ: {sheet_head}...\n"
                    other_sheets_info += f"  - åŠ è½½æ–¹å¼: df_{sheet_name_other} = pd.read_excel('{sheet_path}')\n\n"
            
            # Build prompt with simplified format
            prompt = CODE_GENERATION_TEMPLATE.format(
                question=question,
                excel_path=excel_path,
                original_file=original_file,
                sheet_name=plan.get('sheet_name', ''),
                row_count=row_count,
                column_count=len(columns),
                column_info=column_info,
                head_sample=head_sample,
                tail_sample=tail_sample,
                other_sheets_info=other_sheets_info
            )
            
            # Build system message (may include multi-sheet guidance)
            system_message = """ä½ æ˜¯Pythonæ•°æ®åˆ†æä¸“å®¶ã€‚

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œæä¾›çš„æ•°æ®ç”Ÿæˆåˆ†æä»£ç ã€‚

å…³é”®è§„åˆ™ï¼š
- dfå˜é‡å·²åŒ…å«å½“å‰å·¥ä½œè¡¨çš„æ•°æ®
- ä½¿ç”¨å®é™…çš„åˆ—åï¼ˆä»æ•°æ®è¡¨å¤´ä¸­æŸ¥æ‰¾ï¼‰
- æ ¹æ®æ•°æ®æ ·æœ¬ç†è§£æ¯åˆ—çš„å«ä¹‰ï¼Œé€‰æ‹©æ­£ç¡®çš„åˆ—è¿›è¡Œåˆ†æ
- æ—¥æœŸç±»å‹ç”¨.strftime()è½¬å­—ç¬¦ä¸²
- å¦‚éœ€å›¾è¡¨ï¼Œç”ŸæˆPlotlyå¹¶æ‰“å°JSON"""
            
            # Add multi-sheet guidance if applicable
            if other_sheets and len(other_sheets) > 0:
                system_message += f"""

ã€å¤šå·¥ä½œè¡¨åˆ†æã€‘ï¼š
åŸå§‹æ–‡ä»¶æœ‰{len(other_sheets)+1}ä¸ªå·¥ä½œè¡¨ã€‚
- å¦‚æœé—®é¢˜éœ€è¦è·¨æ—¶é—´æ®µæˆ–è·¨ç±»åˆ«å¯¹æ¯”ï¼Œè€ƒè™‘åŠ è½½å¹¶åˆå¹¶å…¶ä»–å·¥ä½œè¡¨
- ä¾‹å¦‚ï¼š"å„æœˆé”€å”®è¶‹åŠ¿" â†’ å¦‚æœæ¯ä¸ªsheetæ˜¯ä¸€ä¸ªæœˆï¼Œåº”è¯¥åˆå¹¶æ‰€æœ‰sheets
- ä½¿ç”¨pd.read_excel('è·¯å¾„')åŠ è½½å…¶ä»–sheetï¼Œç„¶åpd.concat()åˆå¹¶"""
            
            response = client.chat.completions.create(
                model=self.llm_model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,  # Zero temperature for maximum consistency
                max_tokens=3000,  # Increased token limit for more detailed code
                top_p=0.1,  # Lower top_p for more focused responses
                frequency_penalty=0.0,
                presence_penalty=0.0
            )
            
            code = response.choices[0].message.content
            
            # Extract code from markdown if wrapped
            if '```python' in code:
                code = code.split('```python')[1].split('```')[0].strip()
            elif '```' in code:
                code = code.split('```')[1].split('```')[0].strip()
            
            # Return both code and prompt
            logger.info(f"Generated prompt length: {len(prompt)}")
            return code, prompt
            
        except Exception as e:
            logger.error(f"LLM code generation failed: {e}")
            # No fallback - let the error propagate
            raise e
    
    def generate(self, plan: Dict[str, Any], question: str,
                columns: List[str], types: Dict[str, str],
                data_sample: str = "", row_count: int = 0,
                excel_path: str = "",
                other_sheets: List[Dict] = None,
                original_file: str = "",
                use_llm: bool = True) -> Tuple[str, List[str], str]:
        """
        Generate Python code from plan
        
        Args:
            plan: Executable plan
            question: Original question
            columns: Available columns
            types: Column types
            data_sample: Sample data rows
            row_count: Total row count
            excel_path: Path to clean Excel file
            other_sheets: Other sheets from same file
            original_file: Original file name
            use_llm: Whether to use LLM (default: True for personalized code)
            
        Returns:
            Tuple of (code, expected_columns_used, prompt_used)
        """
        prompt_used = ""
        
        if use_llm and self.openai_api_key:
            # Skip plan generation, let AI generate code directly
            code, prompt_used = self.generate_with_llm_direct(
                question, columns, types, data_sample, row_count, 
                excel_path, other_sheets, original_file
            )
        else:
            # No fallback - require LLM
            raise ValueError("LLM code generation is required but not available")
        
        # Extract expected columns used
        expected_cols = set()
        
        if plan.get('filters'):
            expected_cols.update([f['col'] for f in plan['filters']])
        
        if plan.get('groupby'):
            expected_cols.update(plan['groupby'])
        
        if plan.get('agg'):
            expected_cols.update([a['col'] for a in plan['agg']])
        
        if plan.get('trend'):
            expected_cols.add(plan['trend']['date_col'])
            expected_cols.update(plan['trend'].get('value_cols', []))
        
        if plan.get('sort'):
            expected_cols.update([s['col'] for s in plan['sort']])
        
        if plan.get('text_ops'):
            expected_cols.add(plan['text_ops']['text_col'])
        
        logger.info(f"Generated code ({len(code)} chars), expected columns: {expected_cols}")
        
        return code, list(expected_cols), prompt_used
    
    def generate_price_ranking_code(self, columns: List[str], types: Dict[str, str]) -> str:
        """
        Generate hardcoded code for price ranking analysis
        """
        # Find price column and product column
        price_col = None
        product_col = None
        
        for col in columns:
            if "æ¸…ä»“ä»·" in col:
                price_col = col
            elif "å•†å“åç§°" in col or "äº§å“åç§°" in col:
                product_col = col
        
        if not price_col or not product_col:
            # Fallback to generic code
            return """import pandas as pd
import plotly.express as px

# é€‰æ‹©ç›¸å…³åˆ—
result = df[['å•†å“åç§°', 'æ¸…ä»“ä»·']].copy()

# æ¸…ç†æ•°æ®
result = result.dropna(subset=['æ¸…ä»“ä»·'])

# æŒ‰æ¸…ä»“ä»·æ’åº
result = result.sort_values('æ¸…ä»“ä»·', ascending=True)

# é‡ç½®ç´¢å¼•
result = result.reset_index(drop=True)

# ç”Ÿæˆå›¾è¡¨
fig = px.bar(result, x='å•†å“åç§°', y='æ¸…ä»“ä»·', title='äº§å“æ¸…ä»“ä»·æ ¼æ’å')
print('PLOTLY_JSON:', fig.to_json())

# è¾“å‡ºç»“æœ
print('\\n=== åˆ†æç»“æœ ===')
print(result.to_string(index=False))
print(f'\\næ€»è¡Œæ•°: {len(result)}')"""

        return f"""import pandas as pd
import plotly.express as px

# é€‰æ‹©æ¸…ä»“ä»·å’Œå•†å“åç§°åˆ—
result = df[['{product_col}', '{price_col}']].copy()

# æ¸…ç†æ•°æ®ï¼Œå»é™¤ç©ºå€¼
result = result.dropna(subset=['{price_col}'])

# æŒ‰æ¸…ä»“ä»·æ’åºï¼ˆå‡åºï¼šä»·æ ¼ä»ä½åˆ°é«˜ï¼‰
result = result.sort_values('{price_col}', ascending=True)

# é‡ç½®ç´¢å¼•
result = result.reset_index(drop=True)

# ç”Ÿæˆå›¾è¡¨
fig = px.bar(result, x='{product_col}', y='{price_col}', title='äº§å“æ¸…ä»“ä»·æ ¼æ’å')
print('PLOTLY_JSON:', fig.to_json())

# è¾“å‡ºç»“æœ
print('\\n=== åˆ†æç»“æœ ===')
print(result.to_string(index=False))
print(f'\\næ€»è¡Œæ•°: {len(result)}')"""
    
    def generate_with_llm_direct(self, question: str, columns: List[str], types: Dict[str, str],
                                data_sample: str = "", row_count: int = 0,
                                excel_path: str = "",
                                other_sheets: List[Dict] = None,
                                original_file: str = "") -> Tuple[str, str]:
        """
        Generate code directly using LLM without plan generation
        """
        logger.info("Generating code directly with LLM (no plan)")
        
        # Handle defaults
        if other_sheets is None:
            other_sheets = []
        
        try:
            from openai import OpenAI
            
            client = OpenAI(
                api_key=self.openai_api_key,
                base_url=self.openai_base_url
            )
            
            # Build column info
            column_info = "åˆ—å | ç±»å‹ | ç¤ºä¾‹å€¼\n"
            column_info += "--- | --- | ---\n"
            for col in columns[:10]:  # Limit to first 10 columns
                col_type = types.get(col, 'unknown')
                column_info += f"{col} | {col_type} | è§æ•°æ®æ ·æœ¬\n"
            
            # Build data samples
            head_sample = data_sample.split('\n---\n')[0] if '\n---\n' in data_sample else data_sample[:500]
            tail_sample = data_sample.split('\n---\n')[1] if '\n---\n' in data_sample else ""
            
            # Build other sheets info
            other_sheets_info = ""
            if other_sheets and len(other_sheets) > 0:
                other_sheets_info = f"\nã€åŒä¸€æ–‡ä»¶çš„å…¶ä»–å·¥ä½œè¡¨ã€‘ï¼ˆåŸå§‹æ–‡ä»¶ï¼š{original_file}ï¼Œå…±{len(other_sheets)+1}ä¸ªå·¥ä½œè¡¨ï¼‰\n\n"
                other_sheets_info += "å¦‚æœåˆ†æéœ€è¦ï¼Œå¯ä»¥åŠ è½½å¹¶åˆå¹¶è¿™äº›å·¥ä½œè¡¨çš„æ•°æ®ï¼š\n\n"
                
                for i, sheet_info in enumerate(other_sheets, 1):
                    sheet_name_other = sheet_info['sheet_name']
                    sheet_path = sheet_info['excel_path']
                    sheet_shape = sheet_info['shape']
                    sheet_cols = sheet_info['columns']
                    sheet_head = sheet_info.get('head_sample', '')[:300]
                    
                    other_sheets_info += f"å·¥ä½œè¡¨ {i}: {sheet_name_other}\n"
                    other_sheets_info += f"  - è·¯å¾„: {sheet_path}\n"
                    other_sheets_info += f"  - å¤§å°: {sheet_shape[0]}è¡Œ Ã— {sheet_shape[1]}åˆ—\n"
                    other_sheets_info += f"  - åˆ—å: {', '.join(sheet_cols[:10])}\n"
                    if sheet_head:
                        other_sheets_info += f"  - æ•°æ®é¢„è§ˆ: {sheet_head}...\n"
                    other_sheets_info += f"  - åŠ è½½æ–¹å¼: df_{sheet_name_other} = pd.read_excel('{sheet_path}')\n\n"
            
            # Build prompt with simplified format
            prompt = CODE_GENERATION_TEMPLATE.format(
                question=question,
                excel_path=excel_path,
                original_file=original_file,
                sheet_name="å½“å‰å·¥ä½œè¡¨",
                row_count=row_count,
                column_count=len(columns),
                column_info=column_info,
                head_sample=head_sample,
                tail_sample=tail_sample,
                other_sheets_info=other_sheets_info
            )
            
            # Print the full prompt for debugging
            logger.info("=" * 80)
            logger.info("FULL PROMPT SENT TO AI:")
            logger.info("=" * 80)
            logger.info(prompt)
            logger.info("=" * 80)
            
            # Build system message
            system_message = f"""ä½ æ˜¯Pythonæ•°æ®åˆ†æä¸“å®¶ã€‚

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œæä¾›çš„æ•°æ®ç”Ÿæˆåˆ†æä»£ç ã€‚

âš ï¸ å…³é”®è§„åˆ™ï¼š
- dfå˜é‡å·²åŒ…å«å½“å‰å·¥ä½œè¡¨çš„æ•°æ®ï¼Œä¸è¦å†™pd.read_excel()
- ä½¿ç”¨å®é™…çš„åˆ—åï¼ˆä»æ•°æ®è¡¨å¤´ä¸­æŸ¥æ‰¾ï¼‰
- æ ¹æ®æ•°æ®æ ·æœ¬ç†è§£æ¯åˆ—çš„å«ä¹‰ï¼Œé€‰æ‹©æ­£ç¡®çš„åˆ—è¿›è¡Œåˆ†æ
- æ—¥æœŸç±»å‹ç”¨.strftime()è½¬å­—ç¬¦ä¸²
- å¦‚éœ€å›¾è¡¨ï¼Œç”ŸæˆPlotlyå¹¶æ‰“å°JSON

ğŸ¯ ç‰¹åˆ«é‡è¦ï¼š
- å¯¹äº"æ¸…ä»“ä»·æ ¼æ’å"é—®é¢˜ï¼Œå¿…é¡»ï¼š
  1. é€‰æ‹©"æ¸…ä»“ä»·"åˆ—å’Œ"å•†å“åç§°"åˆ—
  2. ä½¿ç”¨sort_values('æ¸…ä»“ä»·', ascending=True)æ’åº
  3. ç”Ÿæˆå›¾è¡¨å’Œæ•°æ®è¡¨æ ¼
  4. ä¸è¦åªå†™result = df.head(10)

- å¯¹äº"é”€å”®è¶‹åŠ¿"é—®é¢˜ï¼Œå¿…é¡»ï¼š
  1. é€‰æ‹©æ—¥æœŸåˆ—å’Œé”€å”®é¢åˆ—
  2. ä½¿ç”¨groupby()å’Œsum()èšåˆ
  3. ä¸è¦ä½¿ç”¨.size()ï¼Œè¦ç”¨å…·ä½“çš„æ•°å€¼åˆ—

- å¯¹äºæ’åé—®é¢˜ï¼Œå¿…é¡»ä½¿ç”¨sort_values()æ’åº
- å¯¹äºä»·æ ¼æ’åï¼Œé€‰æ‹©ä»·æ ¼åˆ—å’Œäº§å“åç§°åˆ—ï¼ŒæŒ‰ä»·æ ¼æ’åº

å½“å‰é—®é¢˜ï¼š{question}
è¯·ç”Ÿæˆå®Œæ•´çš„ã€å¯ç›´æ¥æ‰§è¡Œçš„Pythonä»£ç ï¼"""
            
            response = client.chat.completions.create(
                model=self.llm_model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=3000,
                top_p=0.1,
                frequency_penalty=0.0,
                presence_penalty=0.0
            )
            
            code = response.choices[0].message.content
            
            # Extract code from markdown if wrapped
            if '```python' in code:
                code = code.split('```python')[1].split('```')[0].strip()
            elif '```' in code:
                code = code.split('```')[1].split('```')[0].strip()
            
            # Ensure required imports are present
            required_imports = [
                "import pandas as pd",
                "import numpy as np", 
                "import plotly.graph_objects as go",
                "import plotly.express as px",
                "import json",
                "import re",
                "from collections import Counter"
            ]
            
            # Check if imports are missing and add them
            code_lines = code.split('\n')
            import_lines = [line for line in code_lines if line.strip().startswith('import') or line.strip().startswith('from')]
            
            if not import_lines:
                # No imports found, add them at the beginning
                code = '\n'.join(required_imports) + '\n\n' + code
                logger.info("Added missing imports to generated code")
            else:
                # Check for missing imports and add them
                missing_imports = []
                for imp in required_imports:
                    if not any(imp in line for line in import_lines):
                        missing_imports.append(imp)
                
                if missing_imports:
                    # Add missing imports after existing imports
                    import_end_idx = 0
                    for i, line in enumerate(code_lines):
                        if line.strip().startswith('import') or line.strip().startswith('from'):
                            import_end_idx = i
                    
                    # Insert missing imports
                    for imp in missing_imports:
                        code_lines.insert(import_end_idx + 1, imp)
                    
                    code = '\n'.join(code_lines)
                    logger.info(f"Added missing imports: {missing_imports}")
            
            # Print the generated code for debugging
            logger.info("=" * 80)
            logger.info("AI GENERATED CODE:")
            logger.info("=" * 80)
            logger.info(code)
            logger.info("=" * 80)
            
            # Validate the code syntax
            try:
                compile(code, '<string>', 'exec')
                logger.info("Code syntax validation passed")
            except SyntaxError as e:
                logger.error(f"Generated code has syntax error: {e}")
                logger.error(f"Error at line {e.lineno}: {e.text}")
                raise e
            
            logger.info(f"Generated code directly ({len(code)} chars)")
            return code, prompt
            
        except Exception as e:
            logger.error(f"Direct LLM code generation failed: {e}")
            # No fallback - let the error propagate
            raise e

